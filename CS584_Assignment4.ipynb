{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Udn_ChCmTzR"
   },
   "source": [
    "# Assignment 4 - Convolutional Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmcezBW5Hzxu",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "In this assignment we will develop a neural network with fully-connected layers to perform classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dvfVkvADtgdu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn import datasets\n",
    "if sys.version_info >= (3, 0):\n",
    "    def xrange(*args, **kwargs):\n",
    "        return iter(range(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "slWD2mKq5SlQ"
   },
   "outputs": [],
   "source": [
    "#load dataset\n",
    "def load_dataset():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    return X, y\n",
    "\n",
    "def train_test_split(X, y):\n",
    "    idx = np.arange(len(X))\n",
    "    train_size = int(len(X) * 2/3)\n",
    "    val_size = int(len(X) * 1/6)\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
    "    y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGq4FfwJHzxx",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "We will use the following class `TwoLayerCNN` to represent instances of our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays. Below, we initialize toy data and a toy model that we will use to develop your implementation. You need to complete the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7p36F_V_Hzxy",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "class TwoLayerCNN(object):\n",
    "  \"\"\"\n",
    "  A two-layer fully-connected neural network. The net has an input dimension of\n",
    "  N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "  We train the network with a softmax loss function and L2 regularization on the\n",
    "  weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "  connected layer.\n",
    "\n",
    "  In other words, the network has the following architecture:\n",
    "\n",
    "  input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "  The outputs of the second fully-connected layer are the scores for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "    \"\"\"\n",
    "    Initialize the model. Weights are initialized to small random values and\n",
    "    biases are initialized to zero. Weights and biases are stored in the\n",
    "    variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "    W1: First layer weights; has shape (D, H)\n",
    "    b1: First layer biases; has shape (H,)\n",
    "    W2: Second layer weights; has shape (H, C)\n",
    "    b2: Second layer biases; has shape (C,)\n",
    "\n",
    "    Inputs:\n",
    "    - input_size: The dimension D of the input data.\n",
    "    - hidden_size: The number of neurons H in the hidden layer.\n",
    "    - output_size: The number of classes C.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "    self.params['b1'] = np.zeros(hidden_size)\n",
    "    self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "    self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "  def loss(self, X, y=None, reg=0.0):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients for a two layer fully connected neural\n",
    "    network.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "      is not passed then we only return scores, and if it is passed then we\n",
    "      instead return the loss and gradients.\n",
    "    - reg: Regularization strength.\n",
    "\n",
    "    Returns:\n",
    "    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "    the score for class c on input X[i].\n",
    "\n",
    "    If y is not None, instead return a tuple of:\n",
    "    - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "      samples.\n",
    "    - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "      with respect to the loss function; has the same keys as self.params.\n",
    "    \"\"\"\n",
    "    # Unpack variables from the params dictionary\n",
    "    W1, b1 = self.params['W1'], self.params['b1']\n",
    "    W2, b2 = self.params['W2'], self.params['b2']\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Compute the forward pass\n",
    "    scores = None\n",
    "    #############################################################################\n",
    "    # Full Mark: 1                                                              #\n",
    "    # TODO: Perform the forward pass, computing the class scores for the input. #\n",
    "    # Store the result in the scores variable, which should be an array of      #\n",
    "    # shape (N, C).                                                             #\n",
    "    #############################################################################\n",
    "\n",
    "    # Using ReLUs as the Activation Function\n",
    "\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W1) + b1)\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    # If the targets are not given then jump out, we're done\n",
    "    if y is None:\n",
    "      return scores\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = None\n",
    "    \n",
    "    scores -= np.max(scores, axis=1, keepdims=True) # avoid numeric instability\n",
    "\n",
    "    #############################################################################\n",
    "    # Full Mark: 2                                                              #\n",
    "    # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
    "    # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
    "    # in the variable loss, which should be a scalar. Use the Softmax           #\n",
    "    # classifier loss.                                                          #\n",
    "    #############################################################################\n",
    "\n",
    "    # softmax\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # [N x K]\n",
    "\n",
    "    # 计算损失\n",
    "    correct_logprobs = -np.log(probs[range(N), y])\n",
    "    data_loss = np.sum(correct_logprobs) / N\n",
    "    reg_loss = 0.5 * reg * np.sum(W1 * W1) + 0.5 * reg * np.sum(W2 * W2)\n",
    "    loss = data_loss + reg_loss\n",
    "\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    grads = {}\n",
    "    #############################################################################\n",
    "    # Full Mark: 2                                                              #\n",
    "    # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "    # and biases. Store the results in the grads dictionary. For example,       #\n",
    "    # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n",
    "    #############################################################################\n",
    "\n",
    "\n",
    "    # 计算scores的梯度\n",
    "    dscores = probs\n",
    "    dscores[range(N), y] -= 1\n",
    "    dscores /= N\n",
    "\n",
    "    # W2 gradient\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "\n",
    "    # b2 gradient\n",
    "    db2 = np.sum(dscores, axis=0)\n",
    "\n",
    "    # 反向传播隐藏层\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    # 反向传播ReLu函数\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    # W1 gradient\n",
    "    dW1 = np.dot(X.T, dhidden)\n",
    "    # b1 gradient\n",
    "    db1 = np.sum(dhidden, axis=0)\n",
    "\n",
    "    # regularization gradient\n",
    "    dW2 += reg * W2\n",
    "    dW1 += reg * W1\n",
    "\n",
    "    # store the results in the grads dictionary\n",
    "    grads = {'W1':dW1, 'b1':db1, 'W2':dW2, 'b2':db2}\n",
    "\n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "  def train(self, X, y, X_val, y_val,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            reg=5e-6, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this neural network using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving training data.\n",
    "    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
    "      X[i] has label c, where 0 <= c < C.\n",
    "    - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "    - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "    - learning_rate: Scalar giving learning rate for optimization.\n",
    "    - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "      after each epoch.\n",
    "    - reg: Scalar giving regularization strength.\n",
    "    - num_iters: Number of steps to take when optimizing.\n",
    "    - batch_size: Number of training examples to use per step.\n",
    "    - verbose: boolean; if true print progress during optimization.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "    # Use SGD to optimize the parameters in self.model\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for it in xrange(num_iters):\n",
    "\n",
    "      #########################################################################\n",
    "      # Full Mark: 0.5                                                        #\n",
    "      # TODO: Create a random minibatch of training data and labels using     #\n",
    "      # given num_train and batch_size, storing them in X_batch and y_batch   #\n",
    "      # respectively.                                                         #\n",
    "      #########################################################################\n",
    "\n",
    "      # 从X中随机选取batch_size个元素\n",
    "      idx = np.random.choice(range(num_train), batch_size if batch_size < num_train else num_train, replace=False)\n",
    "      X_batch = X[idx, :]\n",
    "      y_batch = y[idx] \n",
    "\n",
    "      #########################################################################\n",
    "      #                             END OF YOUR CODE                          #\n",
    "      #########################################################################\n",
    "\n",
    "      # Compute loss and gradients using the current minibatch\n",
    "      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      #########################################################################\n",
    "      # Full Mark: 0.5                                                        #\n",
    "      # TODO: Use the gradients in the grads dictionary to update the         #\n",
    "      # parameters of the network (stored in the dictionary self.params)      #\n",
    "      # using stochastic gradient descent. You'll need to use the gradients   #\n",
    "      # stored in the grads dictionary defined above.                         #\n",
    "      #########################################################################\n",
    "\n",
    "      self.params['W1'] += -learning_rate * grads[\"W1\"]\n",
    "      self.params['W2'] += -learning_rate * grads[\"W2\"]\n",
    "      self.params['b1'] += -learning_rate * grads[\"b1\"]\n",
    "      self.params['b2'] += -learning_rate * grads[\"b2\"]\n",
    "\n",
    "      #########################################################################\n",
    "      #                             END OF YOUR CODE                          #\n",
    "      #########################################################################\n",
    "\n",
    "      if verbose and it % 10 == 0:\n",
    "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "      # Every epoch, check train and val accuracy and decay learning rate.\n",
    "      if it % iterations_per_epoch == 0:\n",
    "        # Check accuracy\n",
    "        train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "        val_acc = (self.predict(X_val) == y_val).mean()\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate *= learning_rate_decay\n",
    "\n",
    "    return {\n",
    "      'loss_history': loss_history,\n",
    "      'train_acc_history': train_acc_history,\n",
    "      'val_acc_history': val_acc_history,\n",
    "    }\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this two-layer network to predict labels for\n",
    "    data points. For each data point we predict scores for each of the C\n",
    "    classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "      classify.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "      to have class c, where 0 <= c < C.\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # Full Mark: 1                                                            #\n",
    "    # TODO: Implement this function                                           #\n",
    "    ###########################################################################\n",
    "\n",
    "    scores = self.loss(X)\n",
    "    # softmax\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # [N x K]\n",
    "    # 反向onehot\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "    ###########################################################################\n",
    "    #                              END OF YOUR CODE                           #\n",
    "    ###########################################################################\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aO2JTi4nmTzd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 0.09086961861319624\n",
      "the test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# To check your implementations.\n",
    "X,y =load_dataset()\n",
    "X_train, y_train, X_val, y_val, X_test, y_test=train_test_split(X, y)\n",
    "\n",
    "###########################################################################\n",
    "# Full Mark: 1                                                            #\n",
    "# TODO: 1. Using TwoLayerCNN to train on given datasets                   #\n",
    "#       2. Print out the final loss                                       #\n",
    "#       3. Print out the test accuracy                                    #\n",
    "###########################################################################\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 1000\n",
    "num_classes = 3\n",
    "net = TwoLayerCNN(input_size, hidden_size, num_classes, std=8e-2)\n",
    "# TODO\n",
    "ret = net.train(X_train, y_train, X_val, y_val,\n",
    "                learning_rate=1e-2,\n",
    "                learning_rate_decay=0.9999,\n",
    "                reg=6e-5,\n",
    "                num_iters=5000,\n",
    "                verbose=False)\n",
    "print(\"final loss:\", ret[\"loss_history\"][-1])\n",
    "acc = (net.predict(X_test) == y_test).mean()\n",
    "print(\"the test accuracy:\", acc)\n",
    "###########################################################################\n",
    "#                              END OF YOUR CODE                           #\n",
    "###########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqWBT4gv-QDS"
   },
   "source": [
    "The loss function and the accuracies on the training and validation sets would give more insight views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YxQumZhV0gLZ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Full Mark: 0.5                                                          #\n",
    "# TODO: Plot training loss history                                        # \n",
    "###########################################################################\n",
    "\n",
    "loss_history = ret[\"loss_history\"]\n",
    "plt.plot(range(len(loss_history)), loss_history, color='r', label='loss')\n",
    "\n",
    "###########################################################################\n",
    "#                              END OF YOUR CODE                           #\n",
    "###########################################################################   \n",
    "    \n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3EC7Ts8L0hDG"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Full Mark: 0.5                                                          #\n",
    "# TODO: Plot Classification accuracy history, compare train/val accuracy  #\n",
    "###########################################################################\n",
    "\n",
    "train_acc_history = ret[\"train_acc_history\"]\n",
    "plt.plot(range(len(train_acc_history)), train_acc_history, color='r', label='train accuracy')\n",
    "val_acc_history = ret[\"val_acc_history\"]\n",
    "plt.plot(range(len(val_acc_history)), val_acc_history, color='b', label='val accuracy')\n",
    "\n",
    "###########################################################################\n",
    "#                              END OF YOUR CODE                           #\n",
    "###########################################################################  \n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2-wpiUvF1G0C"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Full Mark: 1                                                                #\n",
    "# TODO: Describe or using codes to show how you tune your hyperparameters     #\n",
    "# (hidden layer size, learning rate, numer of training epochs, regularization #\n",
    "# strength and so on). Is your result good? Does it look underfiting?         #\n",
    "# Overfiting?                                                                 #\n",
    "###############################################################################\n",
    "\n",
    "hidden_size = 1000\n",
    "net = TwoLayerCNN(input_size, hidden_size, num_classes, std=8e-2)\n",
    "# TODO\n",
    "ret = net.train(X_train, y_train, X_val, y_val,\n",
    "                learning_rate=1e-2,\n",
    "                learning_rate_decay=0.9999,\n",
    "                reg=6e-5,\n",
    "                num_iters=5000)\n",
    "\n",
    "###############################################################################\n",
    "#                              END OF YOUR CODE                               #\n",
    "###############################################################################  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmHbeJjcHzyM",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Explain your hyperparameter tuning process below.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$The hidden layer is too small and the training data fits too poorly, so hidden_size = 1000, too small training times will cause the algorithm to fail to converge, so num_iters=5000, the learning rate is too small and the attenuation factor is too large, causing the algorithm to fail to reach the global optimal solution , So it is necessary to reduce the learning rate and increase the attenuation factor, and to prevent overfitting, the regular term should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS584_Assignment4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}