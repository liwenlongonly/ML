import numpy as np

class NeuralNetwork:
    # ç¥ç»ç½‘ç»œå¤§ç±»
    def __init__(self):
        self._layers = []  # ç½‘ç»œå±‚å¯¹è±¡åˆ—è¡¨

    def add_layer(self, layer):
        # è¿½åŠ ç½‘ç»œå±‚
        self._layers.append(layer)

    # ç½‘ç»œçš„å‰å‘ä¼ æ’­åªéœ€è¦å¾ªç¯è°ƒç”¨ä¸ªç½‘ç»œå±‚å¯¹è±¡çš„å‰å‘è®¡ç®—å‡½æ•°å³å¯
    def feed_forward(self, X):
        # å‰å‘ä¼ æ’­
        for layer in self._layers:
            # ä¾æ¬¡é€šè¿‡å„ä¸ªç½‘ç»œå±‚
            X = layer.activate(X)
        return X

    # ç½‘ç»œæ¨¡å‹çš„åå‘ä¼ æ’­å®ç°ç¨å¤æ‚ï¼Œéœ€è¦ä»æœ€æœ«å±‚å¼€å§‹ï¼Œè®¡ç®—æ¯å±‚çš„ğ›¿å˜é‡ï¼Œæ ¹æ®æˆ‘ä»¬
    # æ¨å¯¼çš„æ¢¯åº¦å…¬å¼ï¼Œå°†è®¡ç®—å‡ºçš„ğ›¿å˜é‡å­˜å‚¨åœ¨Layerç±»çš„deltaå˜é‡ä¸­
    # å› æ­¤ï¼Œåœ¨backpropagation å‡½æ•°ä¸­ï¼Œåå‘è®¡ç®—æ¯å±‚çš„ğ›¿å˜é‡ï¼Œå¹¶æ ¹æ®æ¢¯åº¦å…¬å¼è®¡ç®—æ¯å±‚å‚æ•°çš„æ¢¯åº¦å€¼ï¼Œ
    # æŒ‰ç€æ¢¯åº¦ä¸‹é™ç®—æ³•å®Œæˆä¸€æ¬¡å‚æ•°çš„æ›´æ–°ã€‚
    def backpropagation(self, X, y, learning_rate):

        # åå‘ä¼ æ’­ç®—æ³•å®ç°
        # å‰å‘è®¡ç®—ï¼Œå¾—åˆ°è¾“å‡ºå€¼
        output = self.feed_forward(X)
        for i in reversed(range(len(self._layers))):  # åå‘å¾ªç¯
            layer = self._layers[i]  # å¾—åˆ°å½“å‰å±‚å¯¹è±¡
            # å¦‚æœæ˜¯è¾“å‡ºå±‚
            if layer == self._layers[-1]:  # å¯¹äºè¾“å‡ºå±‚
                layer.error = y - output  # è®¡ç®—2 åˆ†ç±»ä»»åŠ¡çš„å‡æ–¹å·®çš„å¯¼æ•°
                # å…³é”®æ­¥éª¤ï¼šè®¡ç®—æœ€åä¸€å±‚çš„deltaï¼Œå‚è€ƒè¾“å‡ºå±‚çš„æ¢¯åº¦å…¬å¼
                layer.delta = layer.error * layer.apply_activation_derivative(output)

            else:  # å¦‚æœæ˜¯éšè—å±‚
                next_layer = self._layers[i + 1]  # å¾—åˆ°ä¸‹ä¸€å±‚å¯¹è±¡
                layer.error = np.dot(next_layer.weights, next_layer.delta)
                # å…³é”®æ­¥éª¤ï¼šè®¡ç®—éšè—å±‚çš„deltaï¼Œå‚è€ƒéšè—å±‚çš„æ¢¯åº¦å…¬å¼
                layer.delta = layer.error * layer.apply_activation_derivative(layer.last_activation)

        # åœ¨åå‘è®¡ç®—å®Œæ¯å±‚çš„ğ›¿å˜é‡åï¼Œåªéœ€è¦æŒ‰ç€å¼è®¡ç®—æ¯å±‚çš„æ¢¯åº¦ï¼Œå¹¶æ›´æ–°ç½‘ç»œå‚æ•°å³å¯ã€‚
        # ç”±äºä»£ç ä¸­çš„delta è®¡ç®—çš„æ˜¯âˆ’ğ›¿ï¼Œå› æ­¤æ›´æ–°æ—¶ä½¿ç”¨äº†åŠ å·ã€‚
        # å¾ªç¯æ›´æ–°æƒå€¼
        for i in range(len(self._layers)):
            layer = self._layers[i]
            # o_i ä¸ºä¸Šä¸€ç½‘ç»œå±‚çš„è¾“å‡º
            o_i = np.atleast_2d(X if i == 0 else self._layers[i - 1].last_activation)
            # æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œdelta æ˜¯å…¬å¼ä¸­çš„è´Ÿæ•°ï¼Œæ•…è¿™é‡Œç”¨åŠ å·
            layer.weights += layer.delta * o_i.T * learning_rate

    def train(self, X_train, X_test, y_train, y_test, learning_rate, max_epochs):
        # ç½‘ç»œè®­ç»ƒå‡½æ•°
        # one-hot ç¼–ç 
        y_onehot = np.zeros((y_train.shape[0], self._layers[-1].output))
        y_onehot[np.arange(y_train.shape[0]), y_train] = 1
        mses = []
        for i in range(max_epochs):  # è®­ç»ƒ1000 ä¸ªepoch
            for j in range(len(X_train)):  # ä¸€æ¬¡è®­ç»ƒä¸€ä¸ªæ ·æœ¬
                self.backpropagation(X_train[j], y_onehot[j], learning_rate)
            if i % 10 == 0:
                # æ‰“å°å‡ºMSE Loss
                mse = np.mean(np.square(y_onehot - self.feed_forward(X_train)))
                mses.append(mse)
                print('Epoch: #%s, MSE: %f' % (i, float(mse)))
                # ç»Ÿè®¡å¹¶æ‰“å°å‡†ç¡®ç‡
                print('Accuracy: %.2f%%' % (self.accuracy(self.predict(X_test), y_test.flatten()) * 100))
        return mses

    def accuracy(self, y_pre, y_true):
        return np.mean((np.argmax(y_pre, axis=1) == y_true))

    def predict(self, X_test):
        return self.feed_forward(X_test)
