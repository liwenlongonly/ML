{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Multi-class Classification and Neural Network\n",
    "> **FULL MARKS = 10**\n",
    "\n",
    "In this assignment, you are going to implement your own neural network to do multi-class classification. We use one-vs-all strategy here by training multiple binary classifiers (one for each class).\n",
    "\n",
    "Please notice that you can only use numpy and scipy.optimize.minimize. **No** library versions of other method are allowed.  . Follow the instructions, you will need to fill the blanks to make it functional. The process is similar to the previous assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y):\n",
    "    idx = np.arange(len(X))\n",
    "    train_size = int(len(X) * 2/3)\n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(num_in, num_out):\n",
    "    '''\n",
    "    :param num_in: the number of input units in the weight array\n",
    "    :param num_out: the number of output units in the weight array\n",
    "    '''\n",
    "\n",
    "    # Note that 'W' contains both weights and bias, you can consider W[0, :] as bias\n",
    "    W = np.zeros((1 + num_in, num_out))\n",
    "    \n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement Xavier/Glorot uniform initialization                                  #\n",
    "    #                                                                                 #\n",
    "    # Hint: you can find the reference here:                                          #\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform  #\n",
    "    ###################################################################################\n",
    "\n",
    "    # b\n",
    "    b = np.sqrt(6. / num_out)\n",
    "    W[0, :] = np.random.uniform(low=-b, high=b, size=[num_out])\n",
    "    # w\n",
    "    w = np.sqrt(6. / (num_in + num_out))\n",
    "    W[1:, :] = np.random.uniform(low=-w, high=w, size=[num_in, num_out])\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 0.5                                                                  #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement sigmoid function:                                                     #\n",
    "    #                             sigmoid(x) = 1/(1+e^(-x))                           #\n",
    "    ###################################################################################\n",
    "\n",
    "    res = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 0.5                                                                  #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement tanh function:                                                        #\n",
    "    #                     tanh(x) = (e^x-e^(-x)) / (e^x+e^(-x))                       #\n",
    "    ###################################################################################\n",
    "\n",
    "    res = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Computes the gradient of the sigmoid function evaluated at x.                   #\n",
    "    #                                                                                 #\n",
    "    ###################################################################################\n",
    "\n",
    "    s = sigmoid(x)\n",
    "    grad = s * (1 - s)\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_gradient(x):\n",
    "    '''\n",
    "    :param x: input\n",
    "    '''\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Computes the gradient of the tanh function evaluated at x.                      #\n",
    "    #                                                                                 #\n",
    "    ###################################################################################\n",
    "\n",
    "    h = tanh(x)\n",
    "    grad = 1 - np.square(h)\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W, X):\n",
    "    '''\n",
    "    :param W: weights (including biases) of the neural network. It is a list containing both W_hidden and W_output.\n",
    "    :param X: input. Already added one additional column of all \"1\"s.\n",
    "    '''\n",
    "\n",
    "    # Shape of W_hidden: [num_feature+1, num_hidden]\n",
    "    # Shape pf W_output: [num_hidden+1, num_output]\n",
    "    W_hidden, W_output = W\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Implement the forward pass. You need to compute four values:                    #\n",
    "    # z_hidden, a_hidden, z_output, a_output                                          #\n",
    "    #                                                                                 #\n",
    "    # Note that our neural network consists of three layers:                          #\n",
    "    # Input -> Hidden -> Output                                                       #\n",
    "    # The activation function in hidden layer is 'tanh'                               #\n",
    "    # The activation function in output layer is 'sigmoid'                            #\n",
    "    ###################################################################################\n",
    "\n",
    "    z_hidden = X @ W_hidden\n",
    "    x_ = tanh(z_hidden)\n",
    "    a_hidden = np.concatenate([np.ones((len(x_), 1)), x_], axis=1)\n",
    "    z_output = a_hidden @ W_output\n",
    "    a_output = sigmoid(z_output)\n",
    "    \n",
    "    \n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    # z_hidden is the raw output of hidden layer, a_hidden is the result after performing activation on z_hidden\n",
    "    # z_output is the raw output of output layer, a_output is the result after performing activation on z_output\n",
    "    return {'z_hidden': z_hidden, 'a_hidden': a_hidden,\n",
    "            'z_output': z_output, 'a_output': a_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_funtion(W, X, y, num_feature, num_hidden, num_output, L2_lambda):\n",
    "    '''\n",
    "    :param W: a 1D array containing all weights and biases.\n",
    "    :param X: input\n",
    "    :param y: labels of X\n",
    "    :param num_feature: number of features in X\n",
    "    :param num_hidden: number of hidden units\n",
    "    :param num_output: number of output units\n",
    "    :param L2_lambda: the coefficient of regularization term\n",
    "    '''\n",
    "\n",
    "    m = y.size\n",
    "\n",
    "    # Reshape W back into the parameters W_hidden and W_output\n",
    "    W_hidden = np.reshape(W[:num_hidden * (num_feature + 1)],\n",
    "                          ((num_feature + 1), num_hidden))\n",
    "\n",
    "    W_output = np.reshape(W[(num_hidden * (num_feature + 1)):],\n",
    "                          ((num_hidden + 1), num_output))\n",
    "\n",
    "    # Initialize grads\n",
    "    W_hidden_grad = np.zeros(W_hidden.shape)\n",
    "    W_output_grad = np.zeros(W_output.shape)\n",
    "\n",
    "    # Add one column of \"1\"s to X\n",
    "    X_input = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    ##########################################################################################\n",
    "    # Full Mark: 3                                                                           #\n",
    "    # TODO:                                                                                  #\n",
    "    # 1. Transform y to one-hot encoding. Implement binary cross-entropy loss function       #\n",
    "    # (Hint: Use the forward function to get necessary outputs from the model)               #\n",
    "    #                                                                                        #\n",
    "    # 2. Add L2 regularization to all weights in loss                                        #\n",
    "    # (Note that we will not add regularization to bias)                                     #\n",
    "    #                                                                                        #\n",
    "    # 3. Compute the gradient for W_hidden and W_output (including both weights and biases)  #\n",
    "    # (Hint: use chain rule, and the sigmoid gradient, tanh gradient you have                #\n",
    "    # implemented above. Don't forget to add the gradient of regularization term)            #\n",
    "    ##########################################################################################\n",
    "    \n",
    "    y_ = forward([W_hidden, W_output], X_input)\n",
    "    z_hidden = y_[\"z_hidden\"] #(100, 10)\n",
    "    a_hidden = y_[\"a_hidden\"] #(100, 11)\n",
    "    z_output = y_[\"z_output\"] #(100, 3)\n",
    "    a_output = y_[\"a_output\"] #(100, 3)\n",
    "    y_onehot = np.zeros((y.shape[0], num_output))\n",
    "    y_onehot[np.arange(y.shape[0]), y] = 1\n",
    "\n",
    "    data_loss = -np.mean(np.multiply(y_onehot, np.log(a_output)) + np.multiply(1 - y_onehot, np.log(1 - a_output)))\n",
    "    regularization = np.sum(np.square(np.concatenate([W_hidden[1:, :].ravel(), W_output[1:, :].ravel()])))\n",
    "\n",
    "    L = data_loss + (L2_lambda / (2 * m)) * regularization\n",
    "\n",
    "    d_output = a_output - y_onehot # (100, 3)\n",
    "    d_hidden = d_output @ W_output[1:, :].T * tanh_gradient(z_hidden) # (100, 10)\n",
    "    W_output_grad = (1 / m) * a_hidden.T @ d_output # (11, 3)\n",
    "    W_hidden_grad = (1 / m) * X_input.T @ d_hidden # (5, 10)\n",
    "\n",
    "    W_output[0, :] = 0 #(11, 3)\n",
    "    W_hidden[0, :] = 0  # (5, 10)\n",
    "    W_output_grad += (L2_lambda / m) * W_output\n",
    "    W_hidden_grad += (L2_lambda / m) * W_hidden \n",
    "    \n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    grads = np.concatenate([W_hidden_grad.ravel(), W_output_grad.ravel()])\n",
    "\n",
    "    return L, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(initial_W, X, y, num_epoch, num_feature, num_hidden, num_output, L2_lambda):\n",
    "    '''\n",
    "    :param initial_W: initial weights as a 1D array.\n",
    "    :param X: input\n",
    "    :param y: labels of X\n",
    "    :param num_epoch: number of iterations\n",
    "    :param num_feature: number of features in X\n",
    "    :param num_hidden: number of hidden units\n",
    "    :param num_output: number of output units\n",
    "    :param L2_lambda: the coefficient of regularization term\n",
    "    '''\n",
    "\n",
    "    options = {'maxiter': num_epoch}\n",
    "\n",
    "    ###########################################################################################\n",
    "    # Full Mark: 1                                                                            #\n",
    "    # TODO:                                                                                   #\n",
    "    # Optimize weights                                                                        #\n",
    "    # (Hint: use scipy.optimize.minimize to automatically do the iteration.)                  #\n",
    "    # ref: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) #\n",
    "    # For some optimizers, you need to set 'jac' as True.                                     #\n",
    "    # You may need to use lambda to create a function with one parameter to wrap the          #\n",
    "    # loss_funtion you have implemented above.                                                #\n",
    "    #                                                                                         #\n",
    "    # Note that scipy.optimize.minimize only accepts a 1D weight array as initial weights,    #\n",
    "    # and the output optimized weights will also be a 1D array.                               #\n",
    "    # That is why we unroll the initial weights into a single long vector.                    #\n",
    "    ###########################################################################################\n",
    "\n",
    "    def loss(w):\n",
    "        return loss_funtion(w, X, y, num_feature, num_hidden, num_output, L2_lambda)\n",
    "\n",
    "    ret = minimize(fun=loss, x0=initial_W, method=\"SLSQP\", jac=True, options=options)\n",
    "    W_final  = ret.x\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    # Obtain W_hidden and W_output back from W_final\n",
    "    W_hidden = np.reshape(W_final[:num_hidden * (num_feature + 1)],\n",
    "                          ((num_feature + 1), num_hidden))\n",
    "    W_output = np.reshape(W_final[(num_hidden * (num_feature + 1)):],\n",
    "                          ((num_hidden + 1), num_output))\n",
    "\n",
    "    return [W_hidden, W_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, y_test, W):\n",
    "    '''\n",
    "    :param X_test: input\n",
    "    :param y_test: labels of X_test\n",
    "    :param W: a list containing two weights W_hidden and W_output.\n",
    "    '''\n",
    "\n",
    "    test_input = np.concatenate([np.ones((y_test.size, 1)), X_test], axis=1)\n",
    "\n",
    "    ###################################################################################\n",
    "    # Full Mark: 1                                                                    #\n",
    "    # TODO:                                                                           #\n",
    "    # Predict on test set and compute the accuracy.                                   #\n",
    "    # (Hint: use forward function to get predicted output)                            #\n",
    "    #                                                                                 #\n",
    "    ###################################################################################\n",
    "\n",
    "    y_ = forward(W, test_input)\n",
    "    a_output = y_[\"a_output\"]\n",
    "    pred = np.argmax(a_output, axis=1)\n",
    "    acc = np.sum(pred == y_test) / len(y_test)\n",
    "\n",
    "    ###################################################################################\n",
    "    #                       END OF YOUR CODE                                          #\n",
    "    ###################################################################################\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Do not modify this part #\n",
    "# Define parameters\n",
    "NUM_FEATURE = 4\n",
    "NUM_HIDDEN_UNIT = 10\n",
    "NUM_OUTPUT_UNIT = 3\n",
    "NUM_EPOCH = 100\n",
    "L2_lambda = 1\n",
    "\n",
    "# Load data\n",
    "X, y = load_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Initialize weights\n",
    "initial_W_hidden = init_weights(NUM_FEATURE, NUM_HIDDEN_UNIT)\n",
    "initial_W_output = init_weights(NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT)\n",
    "initial_W = np.concatenate([initial_W_hidden.ravel(), initial_W_output.ravel()], axis=0)\n",
    "\n",
    "# Neural network learning\n",
    "W = optimize(initial_W, X_train, y_train, NUM_EPOCH, NUM_FEATURE, NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT, L2_lambda)\n",
    "\n",
    "# Predict\n",
    "acc = predict(X_test, y_test, W)\n",
    "print(\"Test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}